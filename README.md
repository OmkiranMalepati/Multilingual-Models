# Multilingual-Models
Knowledge Distillation - Multilingual BERT

Simple implementation of multilingual models using Knowledge Distillation. Tested for EN, DE and FR in this notebook.
